# =====================================
# Streamlit App: ææˆè¡¨å¤šsheetè‡ªåŠ¨å®¡æ ¸ï¼ˆæ€» + è½»å¡ + é‡å¡ï¼‰
# æ ‡çº¢é”™è¯¯æ ¼ + æ ‡é»„åˆåŒå· + ç²¾ç®€é”™è¯¯ä¸‹è½½ + ç‹¬ç«‹é”™è¯¯æ•°ç»Ÿè®¡
# =====================================
import streamlit as st
import pandas as pd
from io import BytesIO
import unicodedata, re
from openpyxl import Workbook
from openpyxl.styles import PatternFill
from openpyxl.utils.dataframe import dataframe_to_rows

st.title("ğŸ“Š æ¨¡æ‹Ÿäººäº‹ç”¨è–ªèµ„è®¡ç®—è¡¨è‡ªåŠ¨å®¡æ ¸ç³»ç»Ÿ-3")

# ========== ä¸Šä¼ æ–‡ä»¶ ==========
uploaded_files = st.file_uploader(
    "è¯·ä¸Šä¼ æ–‡ä»¶åä¸­åŒ…å«ä»¥ä¸‹å­—æ®µçš„æ–‡ä»¶ï¼šâ€œææˆâ€ã€â€œæ”¾æ¬¾æ˜ç»†â€ã€â€œäºŒæ¬¡æ˜ç»†â€å’Œâ€œåŸè¡¨â€çš„xlsxæ–‡ä»¶ã€‚æœ€åèªŠå†™ï¼Œéœ€æ£€çš„è¡¨ä¸ºææˆè¡¨ã€‚ä½¿ç”¨2æ•°æ®æ—¶ï¼Œæœ¬å¸-1æœˆæ ¼å¼çš„è¡¨éœ€æŒ‰ç…§è¦æ±‚åœ¨æ–‡ä»¶åä¸­åŠ å…¥â€ææˆâ€ï¼Œâ€œåŸè¡¨â€ç­‰åŒºåˆ†ã€‚",
    type="xlsx", accept_multiple_files=True
)

if not uploaded_files or len(uploaded_files) < 4:
    st.warning("âš ï¸ è¯·è‡³å°‘ä¸Šä¼  ææˆè¡¨ã€æ”¾æ¬¾æ˜ç»†ã€äºŒæ¬¡æ˜ç»†ã€åŸè¡¨ å››ä¸ªæ–‡ä»¶")
    st.stop()

# ========== å·¥å…·å‡½æ•° ==========
def find_file(files_list, keyword):
    for f in files_list:
        if keyword in f.name:
            return f
    return None

def normalize_text(val):
    if pd.isna(val):
        return ""
    s = str(val)
    s = re.sub(r'[\n\r\t ]+', '', s)
    s = s.replace('\u3000', '')
    return ''.join(unicodedata.normalize('NFKC', ch) for ch in s).lower().strip()

def normalize_num(val):
    if pd.isna(val):
        return None
    s = str(val).replace(",", "").replace("%", "").strip()
    if s in ["", "-", "nan"]:
        return None
    try:
        return float(s)
    except:
        return None

def find_col(df_like, keyword, exact=False):
    key = keyword.strip().lower()
    columns = df_like.columns if hasattr(df_like, "columns") else df_like.index
    for col in columns:
        cname = str(col).strip().lower()
        if (exact and cname == key) or (not exact and key in cname):
            return col
    return None

def normalize_contract_key(series: pd.Series) -> pd.Series:
    """
    å¯¹åˆåŒå· Series è¿›è¡Œæ ‡å‡†åŒ–å¤„ç†ï¼Œç”¨äºå®‰å…¨çš„ pd.merge æ“ä½œã€‚
    """
    s = series.astype(str)
    s = s.str.replace(r"\.0$", "", regex=True) 
    s = s.str.strip()
    s = s.str.upper() 
    s = s.str.replace('ï¼', '-', regex=False)
    # (è¿™ä¸ªç‰ˆæœ¬ä¸ç§»é™¤å†…éƒ¨ç©ºæ ¼ï¼Œå› ä¸ºåˆåŒå·å¯èƒ½åŒ…å«å®ƒä»¬)
    return s

def prepare_ref_df(df_list, required_cols_dict, prefix):
    """
    (æ–° V2) é¢„å¤„ç†å‚è€ƒDFåˆ—è¡¨ï¼šåˆå¹¶ã€æ ‡å‡†åŒ–Keyã€æå–åˆ—ã€é‡å‘½å
    required_cols_dict: {'åˆåŒ': ('åˆåŒ', False), 'ç±»å‹': ('ç±»å‹', True), ...}
    """
    if not df_list or all(df is None for df in df_list):
        st.warning(f"âš ï¸ {prefix} æ•°æ®åˆ—è¡¨ä¸ºç©ºï¼Œè·³è¿‡é¢„å¤„ç†ã€‚")
        return pd.DataFrame(columns=['__KEY__'])
        
    try:
        df_concat = pd.concat([df for df in df_list if df is not None], ignore_index=True)
    except Exception as e:
        st.error(f"âŒ é¢„å¤„ç† {prefix} æ—¶åˆå¹¶å¤±è´¥: {e}")
        return pd.DataFrame(columns=['__KEY__'])

    # 2. æŸ¥æ‰¾åˆåŒåˆ— (ä»å­—å…¸ä¸­è·å–å…ƒç»„)
    contract_col_kw, contract_exact = required_cols_dict.get('åˆåŒ', ('åˆåŒ', False))
    contract_col = find_col(df_concat, contract_col_kw, exact=contract_exact)
    
    if not contract_col:
        st.warning(f"âš ï¸ åœ¨ {prefix} å‚è€ƒè¡¨ä¸­æœªæ‰¾åˆ°'åˆåŒ'åˆ— (å…³é”®å­—: '{contract_col_kw}', ç²¾ç¡®: {contract_exact})ï¼Œè·³è¿‡æ­¤æ•°æ®æºã€‚")
        return pd.DataFrame(columns=['__KEY__'])
        
    # 3. æå–åˆ— & é‡å‘½å
    cols_to_extract = [contract_col]
    col_mapping = {} # 'åŸå§‹åˆ—å' -> 'ref_prefix_æ ‡å‡†å'
    
    for std_name, (col_kw, is_exact) in required_cols_dict.items(): # <--- V2: è§£åŒ…å…ƒç»„
        if std_name == 'åˆåŒ': continue 
            
        actual_col = find_col(df_concat, col_kw, exact=is_exact) # <--- V2: ä½¿ç”¨ is_exact
        
        if actual_col:
            cols_to_extract.append(actual_col)
            col_mapping[actual_col] = f"ref_{prefix}_{std_name}"
        else:
            # V2: æä¾›æ›´è¯¦ç»†çš„è­¦å‘Š
            st.warning(f"âš ï¸ åœ¨ {prefix} å‚è€ƒè¡¨ä¸­æœªæ‰¾åˆ°åˆ— (å…³é”®å­—: '{col_kw}', ç²¾ç¡®: {is_exact})")
            
    if len(cols_to_extract) == 1: 
        st.warning(f"âš ï¸ åœ¨ {prefix} å‚è€ƒè¡¨ä¸­æœªæ‰¾åˆ°ä»»ä½•æ‰€éœ€å­—æ®µï¼Œè·³è¿‡ã€‚")
        return pd.DataFrame(columns=['__KEY__'])

    # 4. åˆ›å»ºæ ‡å‡†DF
    std_df = df_concat[list(set(cols_to_extract))].copy()
    std_df['__KEY__'] = normalize_contract_key(std_df[contract_col])
    std_df = std_df.rename(columns=col_mapping)
    
    final_cols = ['__KEY__'] + list(col_mapping.values())
    
    # ç¡®ä¿ final_cols éƒ½åœ¨ std_df ä¸­
    final_cols_in_df = [col for col in final_cols if col in std_df.columns]
    std_df = std_df[final_cols_in_df]
    
    # 5. å»é‡
    std_df = std_df.drop_duplicates(subset=['__KEY__'], keep='first')
    return std_df

def compare_series_vec(s_main, s_ref, compare_type='text', tolerance=0, multiplier=1):
    """
    (æ–°) å‘é‡åŒ–æ¯”è¾ƒå‡½æ•°ï¼Œå¤åˆ»æ‰€æœ‰ä¸šåŠ¡é€»è¾‘ã€‚
    """
    # 0. è¯†åˆ« Merge å¤±è´¥
    merge_failed_mask = s_ref.isna()

    # 1. é¢„å¤„ç†ç©ºå€¼
    main_is_na = pd.isna(s_main) | (s_main.astype(str).str.strip().isin(["", "nan", "None"]))
    ref_is_na = pd.isna(s_ref) | (s_ref.astype(str).str.strip().isin(["", "nan", "None"]))
    both_are_na = main_is_na & ref_is_na
    
    errors = pd.Series(False, index=s_main.index)

    # 2. æ—¥æœŸæ¯”è¾ƒ
    if compare_type == 'date':
        d_main = pd.to_datetime(s_main, errors='coerce').dt.normalize()
        d_ref = pd.to_datetime(s_ref, errors='coerce').dt.normalize()
        
        valid_dates_mask = d_main.notna() & d_ref.notna()
        date_diff_mask = (d_main != d_ref)
        errors = valid_dates_mask & date_diff_mask
        
        one_is_date_one_is_not = (d_main.notna() & d_ref.isna() & ~ref_is_na) | \
                                 (d_main.isna() & ~main_is_na & d_ref.notna())
        errors |= one_is_date_one_is_not

    # 3. æ•°å€¼æ¯”è¾ƒ
    elif compare_type == 'num' or compare_type == 'rate' or compare_type == 'term':
        s_main_norm = s_main.apply(normalize_num)
        s_ref_norm = s_ref.apply(normalize_num)
        
        # ç‰¹æ®Šï¼šæ”¶ç›Šç‡ï¼ˆç™¾åˆ†æ¯”/å°æ•°ï¼‰
        if compare_type == 'rate':
            s_main_norm = s_main_norm.apply(lambda x: x / 100 if (x is not None and x > 1) else x)
            s_ref_norm = s_ref_norm.apply(lambda x: x / 100 if (x is not None and x > 1) else x)
            
        # ç‰¹æ®Šï¼šæœŸé™ï¼ˆä¹˜æ•°ï¼‰
        if compare_type == 'term':
            s_ref_norm = pd.to_numeric(s_ref_norm, errors='coerce') * multiplier

        is_num_main = s_main_norm.apply(lambda x: isinstance(x, (int, float)))
        is_num_ref = s_ref_norm.apply(lambda x: isinstance(x, (int, float)))
        both_are_num = is_num_main & is_num_ref

        if both_are_num.any():
            diff = (s_main_norm[both_are_num] - s_ref_norm[both_are_num]).abs()
            errors.loc[both_are_num] = (diff > (tolerance + 1e-6))
            
        one_is_num_one_is_not = (is_num_main & ~is_num_ref & ~ref_is_na) | \
                                (~is_num_main & ~main_is_na & is_num_ref)
        errors |= one_is_num_one_is_not

    # 4. æ–‡æœ¬æ¯”è¾ƒ
    else: # compare_type == 'text'
        s_main_norm_text = s_main.apply(normalize_text)
        s_ref_norm_text = s_ref.apply(normalize_text)
        errors = (s_main_norm_text != s_ref_norm_text)

    # 5. æœ€ç»ˆé”™è¯¯é€»è¾‘
    final_errors = errors & ~both_are_na
    lookup_failure_mask = merge_failed_mask & ~main_is_na
    final_errors = final_errors & ~lookup_failure_mask
    
    return final_errors

# ========== è¯»å–æ–‡ä»¶ & é¢„å¤„ç† ==========
st.info("â„¹ï¸ æ­£åœ¨è¯»å–å¹¶é¢„å¤„ç†æ‰€æœ‰æ–‡ä»¶...")

tc_file = find_file(uploaded_files, "ææˆ")
fk_file = find_file(uploaded_files, "æ”¾æ¬¾æ˜ç»†")
ec_file = find_file(uploaded_files, "äºŒæ¬¡æ˜ç»†")
original_file = find_file(uploaded_files, "åŸè¡¨")

# 1. è¯»å–ä¸»è¡¨ (ææˆ)
# ... (è¿™éƒ¨åˆ†ä¸å˜, ä¿æŒåŸæ ·) ...
tc_xls = pd.ExcelFile(tc_file)
sheet_total = next((s for s in tc_xls.sheet_names if "æ€»" in s), None)
sheets_qk = [s for s in tc_xls.sheet_names if "è½»å¡" in s]
sheets_zk = [s for s in tc_xls.sheet_names if "é‡å¡" in s]

tc_sheets = {
    "æ€»": [pd.read_excel(tc_file, sheet_name=sheet_total)] if sheet_total else [],
    "è½»å¡": [pd.read_excel(tc_file, sheet_name=s) for s in sheets_qk],
    "é‡å¡": [pd.read_excel(tc_file, sheet_name=s) for s in sheets_zk],
}

# 2. è¯»å–å¹¶é¢„å¤„ç†å‚è€ƒè¡¨
# --- æ”¾æ¬¾æ˜ç»† (fk) ---
fk_xls = pd.ExcelFile(fk_file)
fk_dfs_raw = [pd.read_excel(fk_file, sheet_name=s) for s in fk_xls.sheet_names if "æ½®æ£" in s]

# --- VVVV (ã€æ ¸å¿ƒä¿®æ”¹ã€‘æ›´æ–°ä¸º (å…³é”®å­—, æ˜¯å¦ç²¾ç¡®) çš„å…ƒç»„) VVVV ---
fk_cols_needed = {
    # {æ ‡å‡†å: (å…³é”®å­—, ç²¾ç¡®åŒ¹é…)}
    'åˆåŒ': ('åˆåŒ', False),
    'æ”¾æ¬¾æ—¥æœŸ': ('æ”¾æ¬¾æ—¥æœŸ', False),
    'ææŠ¥äººå‘˜': ('ææŠ¥äººå‘˜', False),
    'åŸå¸‚ç»ç†': ('åŸå¸‚ç»ç†', False),
    'ç§Ÿèµæœ¬é‡‘': ('ç§Ÿèµæœ¬é‡‘', False),
    'xirr': ('xirr', False),
    'ç§ŸèµæœŸé™/å¹´': ('ç§ŸèµæœŸé™/å¹´', False), # æ¨¡ç³ŠåŒ¹é…ä¹ŸOKï¼Œä½†ç²¾ç¡®æ›´å¥½
    'å®¶è®¿': ('å®¶è®¿', False),
    'ç±»å‹': ('ç±»å‹', True) # <--- è¿™å°±æ˜¯æ‚¨è¦çš„ä¿®å¤ï¼
}
fk_std = prepare_ref_df(fk_dfs_raw, fk_cols_needed, "fk")

# --- äºŒæ¬¡æ˜ç»† (ec) ---
ec_xls = pd.ExcelFile(ec_file)
ec_dfs_raw = [pd.read_excel(ec_file, sheet_name=s) for s in ec_xls.sheet_names]
ec_cols_needed = {
    'åˆåŒ': ('åˆåŒ', False),
    'å‡ºæœ¬æµç¨‹æ—¶é—´': ('å‡ºæœ¬æµç¨‹æ—¶é—´', False)
}
ec_std = prepare_ref_df(ec_dfs_raw, ec_cols_needed, "ec")

# --- åŸè¡¨ (original) ---
original_dfs_raw = [pd.read_excel(original_file)]
original_cols_needed = {
    'åˆåŒ': ('åˆåŒ', False),
    'å¹´åŒ–nim': ('å¹´åŒ–nim', False)
}
orig_std = prepare_ref_df(original_dfs_raw, original_cols_needed, "original")
# --- ^^^^ (ä¿®æ”¹ç»“æŸ) ^^^^ ---

all_std_dfs = {
    "fk": fk_std,
    "ec": ec_std,
    "orig": orig_std
}

st.success(f"âœ… ææˆè¡¨å·²è¯»å–ï¼šæ€»({len(tc_sheets['æ€»'])})ã€è½»å¡({len(tc_sheets['è½»å¡'])})ã€é‡å¡({len(tc_sheets['é‡å¡'])})")
st.success("âœ… æ‰€æœ‰å‚è€ƒæ–‡ä»¶å·²é¢„å¤„ç†å®Œæˆã€‚")

# 3. å®šä¹‰ MAPPING (ä¿æŒä¸å˜)
MAPPING = {
    "æ”¾æ¬¾æ—¥æœŸ": ("æ”¾æ¬¾æ˜ç»†", "æ”¾æ¬¾æ—¥æœŸ", 0, 1),
    "ææŠ¥äººå‘˜": ("æ”¾æ¬¾æ˜ç»†", "ææŠ¥äººå‘˜", 0, 1),
    "åŸå¸‚ç»ç†": ("æ”¾æ¬¾æ˜ç»†", "åŸå¸‚ç»ç†", 0, 1),
    "ç§Ÿèµæœ¬é‡‘": ("æ”¾æ¬¾æ˜ç»†", "ç§Ÿèµæœ¬é‡‘", 0, 1),
    "æ”¶ç›Šç‡": ("æ”¾æ¬¾æ˜ç»†", "xirr", 0.01, 1), 
    "æœŸé™": ("æ”¾æ¬¾æ˜ç»†", "ç§ŸèµæœŸé™/å¹´", 0.5, 12),
    "å®¶è®¿": ("æ”¾æ¬¾æ˜ç»†", "å®¶è®¿", 0, 1),
    "äººå‘˜ç±»å‹": ("æ”¾æ¬¾æ˜ç»†", "ç±»å‹", 0, 1),
    "äºŒæ¬¡äº¤æ¥": ("äºŒæ¬¡æ˜ç»†", "å‡ºæœ¬æµç¨‹æ—¶é—´", 0, 1),
}

# =====================================
# ğŸ§® æ ¸å¿ƒå®¡æ ¸å‡½æ•° (å‘é‡åŒ–ç‰ˆ)
# =====================================
def audit_one_sheet_vec(tc_df, sheet_label, all_std_dfs):
    contract_col_main = find_col(tc_df, "åˆåŒ")
    if not contract_col_main:
        st.warning(f"âš ï¸ {sheet_label}ï¼šæœªæ‰¾åˆ°â€˜åˆåŒâ€™åˆ—ï¼Œè·³è¿‡ã€‚")
        return None, None, 0, 0
    
    # 1. å‡†å¤‡ä¸»è¡¨
    tc_df['__ROW_IDX__'] = tc_df.index
    tc_df['__KEY__'] = normalize_contract_key(tc_df[contract_col_main])

    # 2. ä¸€æ¬¡æ€§åˆå¹¶æ‰€æœ‰å‚è€ƒæ•°æ®
    merged_df = tc_df.copy()
    for std_df in all_std_dfs.values():
        if not std_df.empty:
            merged_df = pd.merge(merged_df, std_df, on='__KEY__', how='left')

    # 3. === éå†å­—æ®µè¿›è¡Œå‘é‡åŒ–æ¯”å¯¹ ===
    total_errors = 0
    errors_locations = set() # å­˜å‚¨ (row_idx, col_name)
    row_has_error = pd.Series(False, index=merged_df.index)

    progress = st.progress(0)
    status = st.empty()
    n = len(merged_df)

    for i, (main_kw, (src, ref_kw, tol, mult)) in enumerate(MAPPING.items()):
        
        exact_main = "æœŸé™" in main_kw or main_kw == "äººå‘˜ç±»å‹"
        main_col = find_col(merged_df, main_kw, exact=exact_main)
        if not main_col:
            continue
            
        status.text(f"{sheet_label} å®¡æ ¸è¿›åº¦ï¼š{i+1}/{len(MAPPING)} - {main_kw}")
        
        s_main = merged_df[main_col]
        
        # 4. === (æ ¸å¿ƒ) å¤„ç†æ¡ä»¶é€»è¾‘ ===
        if main_kw == "æ”¶ç›Šç‡":
            person_type_col = find_col(merged_df, "äººå‘˜ç±»å‹", exact=True)
            if not person_type_col:
                continue # æ— æ³•åˆ¤æ–­ç±»å‹ï¼Œè·³è¿‡
                
            s_ref_fk = merged_df.get('ref_fk_xirr') # æ”¾æ¬¾æ˜ç»†
            s_ref_orig = merged_df.get('ref_orig_å¹´åŒ–nim') # åŸè¡¨
            
            # é»˜è®¤ä½¿ç”¨æ”¾æ¬¾æ˜ç»†
            s_ref_final = s_ref_fk.copy()
            
            # å¦‚æœç±»å‹ä¸º"è½»å¡", åˆ™è¦†ç›–ä¸º"åŸè¡¨"çš„å€¼
            if s_ref_orig is not None:
                mask_light_truck = (merged_df[person_type_col].astype(str).str.strip() == "è½»å¡")
                s_ref_final.loc[mask_light_truck] = s_ref_orig.loc[mask_light_truck]
            
            errors_mask = compare_series_vec(s_main, s_ref_final, compare_type='rate', tolerance=tol)
        
        elif "æ—¥æœŸ" in main_kw or main_kw == "äºŒæ¬¡äº¤æ¥":
            ref_col_name = f"ref_{'ec' if src == 'äºŒæ¬¡æ˜ç»†' else 'fk'}_{ref_kw}"
            s_ref = merged_df.get(ref_col_name)
            errors_mask = compare_series_vec(s_main, s_ref, compare_type='date')
            
        elif "æœŸé™" in main_kw:
            ref_col_name = f"ref_fk_{ref_kw}"
            s_ref = merged_df.get(ref_col_name)
            errors_mask = compare_series_vec(s_main, s_ref, compare_type='term', tolerance=tol, multiplier=mult)

        elif main_kw in ["ç§Ÿèµæœ¬é‡‘", "å®¶è®¿"]: # å…¶ä»–æ•°å€¼
            ref_col_name = f"ref_fk_{ref_kw}"
            s_ref = merged_df.get(ref_col_name)
            errors_mask = compare_series_vec(s_main, s_ref, compare_type='num', tolerance=tol)

        else: # æ–‡æœ¬
            ref_col_name = f"ref_fk_{ref_kw}"
            s_ref = merged_df.get(ref_col_name)
            errors_mask = compare_series_vec(s_main, s_ref, compare_type='text')
            
        # 5. ç´¯ç§¯é”™è¯¯
        if errors_mask is not None and errors_mask.any():
            total_errors += errors_mask.sum()
            row_has_error |= errors_mask
            
            bad_indices = merged_df[errors_mask]['__ROW_IDX__']
            for idx in bad_indices:
                errors_locations.add((idx, main_col))
                
        progress.progress((i + 1) / len(MAPPING))

    status.text(f"{sheet_label} æ¯”å¯¹å®Œæˆï¼Œæ­£åœ¨ç”Ÿæˆæ ‡æ³¨æ–‡ä»¶...")

    # 6. === å¿«é€Ÿå†™å…¥ Excel (æ›¿æ¢æ—§çš„å¾ªç¯) ===
    wb = Workbook()
    ws = wb.active
    red_fill = PatternFill(start_color="FFC7CE", end_color="FFC7CE", fill_type="solid")
    yellow_fill = PatternFill(start_color="FFFF00", end_color="FFFF00", fill_type="solid")

    # å‡†å¤‡åŸå§‹åˆ—
    original_cols_list = list(tc_df.drop(columns=['__ROW_IDX__', '__KEY__']).columns)
    col_name_to_idx = {name: i + 1 for i, name in enumerate(original_cols_list)}

    # å†™å…¥è¡¨å¤´ + æ•°æ®
    for r in dataframe_to_rows(merged_df[original_cols_list], index=False, header=True):
        ws.append(r)
    
    # æ ‡çº¢
    for (row_idx, col_name) in errors_locations:
        if col_name in col_name_to_idx:
            excel_row = row_idx + 2 # (row_idx 0-based) + (1 for header) + (1 for 1-based)
            excel_col = col_name_to_idx[col_name]
            ws.cell(excel_row, excel_col).fill = red_fill
            
    # æ ‡é»„
    if contract_col_main in col_name_to_idx:
        contract_col_excel_idx = col_name_to_idx[contract_col_main]
        error_row_indices = merged_df[row_has_error]['__ROW_IDX__']
        for row_idx in error_row_indices:
            excel_row = row_idx + 2
            ws.cell(excel_row, contract_col_excel_idx).fill = yellow_fill

    output_full = BytesIO()
    wb.save(output_full)
    output_full.seek(0)
    
    # 7. === (æ–°) å¿«é€Ÿç”Ÿæˆç²¾ç®€é”™è¯¯è¡¨ ===
    output_err = BytesIO()
    error_row_count = row_has_error.sum()
    
    if error_row_count > 0:
        try:
            df_errors_only = merged_df.loc[row_has_error, original_cols_list].copy()
            
            original_indices_with_error = merged_df.loc[row_has_error, '__ROW_IDX__']
            original_idx_to_new_excel_row = {
                original_idx: new_row_num 
                for new_row_num, original_idx in enumerate(original_indices_with_error, start=2)
            }

            wb_err = Workbook()
            ws_err = wb_err.active
            
            for r in dataframe_to_rows(df_errors_only, index=False, header=True):
                ws_err.append(r)
                
            for (original_row_idx, col_name) in errors_locations:
                if original_row_idx in original_idx_to_new_excel_row:
                    new_row = original_idx_to_new_excel_row[original_row_idx]
                    if col_name in col_name_to_idx:
                        new_col = col_name_to_idx[col_name]
                        ws_err.cell(row=new_row, column=new_col).fill = red_fill
            
            # æ ‡é»„åˆåŒå·
            contract_col_excel_idx = col_name_to_idx[contract_col_main]
            for new_row_num in original_idx_to_new_excel_row.values():
                 ws_err.cell(row=new_row_num, column=contract_col_excel_idx).fill = yellow_fill

            wb_err.save(output_err)
            output_err.seek(0)
        except Exception as e:
            st.error(f"âŒ ç”Ÿæˆâ€œé”™è¯¯ç²¾ç®€ç‰ˆâ€æ–‡ä»¶æ—¶å‡ºé”™: {e}")
            output_err = None # è®¾ä¸ºNone
    else:
        output_err = None # æ²¡æœ‰é”™è¯¯
        
    return output_full, output_err, total_errors, error_row_count
    
# ========== å®¡æ ¸æ‰€æœ‰ sheet ==========
results = {}
for label, df_list in tc_sheets.items():
    if not df_list:
        continue
    for i, df in enumerate(df_list, start=1):
        tag = f"{label}{i if len(df_list) > 1 else ''}"
        st.divider()
        st.subheader(f"ğŸ“˜ æ­£åœ¨å®¡æ ¸ï¼š{tag}")
        full, err, errs, rows = audit_one_sheet_vec(df, tag, all_std_dfs)
        results[tag] = (full, err, errs, rows)

# ========== ğŸ” åå‘æ¼å¡«æ£€æŸ¥ï¼ˆä½¿ç”¨æ ‡å‡†åŒ–Keyï¼‰ ==========
st.divider()
st.subheader("ğŸ” åå‘æ¼å¡«æ£€æŸ¥ï¼ˆä»…åŸºäºæ”¾æ¬¾æ˜ç»†ä¸­åŒ…å«â€œæ½®æ£â€çš„sheetï¼‰")

# 1. ä» "æ€»" sheet è·å–æ ‡å‡†åˆåŒå·
contracts_total = set()
if tc_sheets["æ€»"]:
    df_total = tc_sheets["æ€»"][0]
    col = find_col(df_total, "åˆåŒ", exact=False)
    if col is not None:
        contracts_total = set(normalize_contract_key(df_total[col].dropna()))

# 2. ä»é¢„å¤„ç†çš„ fk_std DataFrame è·å–æ ‡å‡†åˆåŒå·
# (è¿™æ¯”é‡æ–°è¯»å– fk_dfs è¦å¿«å¾—å¤šï¼Œä¸”å·²æ ‡å‡†åŒ–)
contracts_fk = set(fk_std['__KEY__'].dropna())

missing_contracts = sorted(list(contracts_fk - contracts_total))

if missing_contracts:
    st.warning(f"âš ï¸ å‘ç° {len(missing_contracts)} ä¸ªåˆåŒå·å­˜åœ¨äºæ”¾æ¬¾æ˜ç»†ä¸­ï¼Œä½†æœªå‡ºç°åœ¨ææˆè¡¨â€˜æ€»â€™sheetä¸­")
    df_missing = pd.DataFrame({"æ¼å¡«åˆåŒå·": missing_contracts})
    output_missing = BytesIO()
    
    # (ä½¿ç”¨ openpyxl å†™å…¥ï¼Œé¿å…é¢å¤–çš„ pd.ExcelWriter ä¾èµ–)
    wb_miss = Workbook()
    ws_miss = wb_miss.active
    ws_miss.cell(1, 1, "æ¼å¡«åˆåŒå·")
    for r, contract in enumerate(missing_contracts, start=2):
        ws_miss.cell(r, 1, contract)
        
    wb_miss.save(output_missing)
    output_missing.seek(0)
    
    st.download_button(
        "ğŸ“¥ ä¸‹è½½æ¼å¡«åˆåŒå·è¡¨ï¼ˆåŸºäºæ”¾æ¬¾æ˜ç»†-æ½®æ£ï¼‰",
        data=output_missing,
        file_name="ææˆ_æ¼å¡«åˆåŒå·_åŸºäºæ”¾æ¬¾æ˜ç»†_æ½®æ£.xlsx",
        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
    )
else:
    st.success("âœ… æœªå‘ç°æ¼å¡«åˆåŒå·ï¼ˆåŸºäºæ”¾æ¬¾æ˜ç»†-æ½®æ£ï¼‰ã€‚")

# ========== ä¸‹è½½åŒº ==========
st.divider()
st.subheader("ğŸ“¤ ä¸‹è½½å®¡æ ¸ç»“æœæ–‡ä»¶")

for tag, (full, err, errs, rows) in results.items():
    st.write(f"ğŸ“˜ **{tag}**ï¼šå‘ç° {errs} ä¸ªé”™è¯¯ï¼Œå…± {rows} è¡Œå¼‚å¸¸")
    st.download_button(
        f"ğŸ“¥ ä¸‹è½½ {tag} å®¡æ ¸æ ‡æ³¨ç‰ˆ",
        data=full,
        file_name=f"ææˆ_{tag}_å®¡æ ¸æ ‡æ³¨ç‰ˆ.xlsx",
        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        key=f"download_full_{tag}" # ç¡®ä¿Keyå”¯ä¸€
    )
    
    # --- VVVV (æ·»åŠ æ£€æŸ¥) VVVV ---
    if rows > 0 and err is not None:
    # --- ^^^^ (æ·»åŠ æ£€æŸ¥) ^^^^ ---
        st.download_button(
            f"ğŸ“¥ ä¸‹è½½ {tag} é”™è¯¯ç²¾ç®€ç‰ˆï¼ˆå«çº¢é»„æ ‡è®°ï¼‰",
            data=err,
            file_name=f"ææˆ_{tag}_é”™è¯¯ç²¾ç®€ç‰ˆ.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            key=f"download_err_{tag}" # ç¡®ä¿Keyå”¯ä¸€
        )

st.success("âœ… æ‰€æœ‰sheetå®¡æ ¸å®Œæˆï¼")
